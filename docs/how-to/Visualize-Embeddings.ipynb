{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing LLM Embeddings\n",
    "\n",
    "This notebook demonstrates how to visualize embeddings from indexed LinkML-Store collections using dimensionality reduction techniques like UMAP and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml_store import Client\n",
    "from linkml_store.index.implementations.llm_indexer import LLMIndexer\n",
    "from linkml_store.utils.embedding_utils import (\n",
    "    extract_embeddings_from_collection,\n",
    "    extract_embeddings_from_multiple_collections,\n",
    "    compute_embedding_statistics\n",
    ")\n",
    "from linkml_store.plotting.dimensionality_reduction import (\n",
    "    reduce_dimensions,\n",
    "    get_optimal_parameters\n",
    ")\n",
    "from linkml_store.plotting.embedding_plot import (\n",
    "    plot_embeddings,\n",
    "    plot_embeddings_comparison,\n",
    "    EmbeddingPlotConfig\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Data\n",
    "\n",
    "Let's create two collections with different types of documents and index them with LLM embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 papers and 5 news articles\n"
     ]
    }
   ],
   "source": [
    "# Create a client and database\n",
    "client = Client()\n",
    "db = client.get_database(\"duckdb:///:memory:\")\n",
    "\n",
    "# Create first collection: Scientific papers\n",
    "papers = db.get_collection(\"papers\")\n",
    "papers_data = [\n",
    "    {\n",
    "        \"id\": \"paper1\",\n",
    "        \"title\": \"Deep Learning for Natural Language Processing\",\n",
    "        \"category\": \"AI\",\n",
    "        \"year\": 2023,\n",
    "        \"abstract\": \"This paper explores recent advances in deep learning models for NLP tasks.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper2\",\n",
    "        \"title\": \"Quantum Computing Applications in Cryptography\",\n",
    "        \"category\": \"Quantum\",\n",
    "        \"year\": 2023,\n",
    "        \"abstract\": \"We investigate the implications of quantum computing for modern cryptographic systems.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper3\",\n",
    "        \"title\": \"Climate Change Impact on Marine Ecosystems\",\n",
    "        \"category\": \"Climate\",\n",
    "        \"year\": 2024,\n",
    "        \"abstract\": \"Analysis of temperature changes affecting coral reefs and marine biodiversity.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper4\",\n",
    "        \"title\": \"Transformer Architectures for Computer Vision\",\n",
    "        \"category\": \"AI\",\n",
    "        \"year\": 2024,\n",
    "        \"abstract\": \"Adapting transformer models from NLP to solve computer vision problems.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"paper5\",\n",
    "        \"title\": \"Sustainable Energy Solutions for Urban Areas\",\n",
    "        \"category\": \"Climate\",\n",
    "        \"year\": 2023,\n",
    "        \"abstract\": \"Exploring renewable energy integration in modern city infrastructure.\"\n",
    "    }\n",
    "]\n",
    "papers.insert(papers_data)\n",
    "\n",
    "# Create second collection: News articles\n",
    "news = db.get_collection(\"news\")\n",
    "news_data = [\n",
    "    {\n",
    "        \"id\": \"news1\",\n",
    "        \"headline\": \"Tech Giant Announces New AI Assistant\",\n",
    "        \"topic\": \"Technology\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"content\": \"Major technology company reveals advanced AI assistant with multimodal capabilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news2\",\n",
    "        \"headline\": \"Stock Market Reaches Record High\",\n",
    "        \"topic\": \"Finance\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"content\": \"Markets surge as investors show confidence in economic recovery.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news3\",\n",
    "        \"headline\": \"New Climate Agreement Signed by Nations\",\n",
    "        \"topic\": \"Environment\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"content\": \"Countries commit to reducing emissions by 50% over the next decade.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news4\",\n",
    "        \"headline\": \"Healthcare Breakthrough in Cancer Treatment\",\n",
    "        \"topic\": \"Health\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"content\": \"Researchers develop new immunotherapy showing promising results.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news5\",\n",
    "        \"headline\": \"Cybersecurity Threats on the Rise\",\n",
    "        \"topic\": \"Technology\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"content\": \"Experts warn of increasing sophisticated attacks targeting infrastructure.\"\n",
    "    }\n",
    "]\n",
    "news.insert(news_data)\n",
    "\n",
    "print(f\"Created {papers.find().num_rows} papers and {news.find().num_rows} news articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Collections\n",
    "\n",
    "Now let's create LLM indexes for both collections. Note: This requires an OpenAI API key or another configured LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections indexed successfully\n"
     ]
    }
   ],
   "source": [
    "# For demo purposes, we'll use a simple indexer\n",
    "# In production, use LLMIndexer with proper API configuration\n",
    "from linkml_store.index.implementations.simple_indexer import SimpleIndexer\n",
    "\n",
    "# Index papers collection\n",
    "papers_indexer = SimpleIndexer(\n",
    "    name=\"semantic\",\n",
    "    text_template=\"{title} {abstract}\",\n",
    "    dimensions=100  # Reduced for demo\n",
    ")\n",
    "papers.attach_indexer(papers_indexer)\n",
    "\n",
    "# Index news collection\n",
    "news_indexer = SimpleIndexer(\n",
    "    name=\"semantic\",\n",
    "    text_template=\"{headline} {content}\",\n",
    "    dimensions=100  # Reduced for demo\n",
    ")\n",
    "news.attach_indexer(news_indexer)\n",
    "\n",
    "print(\"Collections indexed successfully\")\n",
    "\n",
    "# For real LLM indexing (requires API key):\n",
    "# papers_indexer = LLMIndexer(\n",
    "#     name=\"semantic\",\n",
    "#     text_template=\"{title} {abstract}\",\n",
    "#     cached_embeddings_database=\"embeddings_cache.db\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings\n",
    "\n",
    "Let's extract embeddings from both collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 embeddings\n",
      "Embedding dimensions: 1000\n",
      "\n",
      "Embedding statistics:\n",
      "  n_samples: 10\n",
      "  n_dimensions: 1000\n",
      "  n_collections: 2\n",
      "  collections: ['papers', 'news']\n",
      "  samples_per_collection: {'papers': 5, 'news': 5}\n",
      "  mean_norm: 1.000\n",
      "  std_norm: 0.000\n",
      "  mean_similarity: 0.189\n",
      "  std_similarity: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings from multiple collections\n",
    "embedding_data = extract_embeddings_from_multiple_collections(\n",
    "    database=db,\n",
    "    collection_names=[\"papers\", \"news\"],\n",
    "    index_name=\"semantic\",\n",
    "    include_metadata=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(f\"Extracted {embedding_data.n_samples} embeddings\")\n",
    "print(f\"Embedding dimensions: {embedding_data.n_dimensions}\")\n",
    "\n",
    "# Compute statistics\n",
    "stats = compute_embedding_statistics(embedding_data)\n",
    "print(\"\\nEmbedding statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Now let's reduce the high-dimensional embeddings to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: Reduced to 2 dimensions\n",
      "  Explained variance: 27.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cjm/Library/Caches/pypoetry/virtualenvs/linkml-store-8ZYO4kTy-py3.10/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE: Reduced to 2 dimensions\n",
      "UMAP: Skipped - umap-learn is required for UMAP. Install with: pip install umap-learn\n"
     ]
    }
   ],
   "source": [
    "# Try different reduction methods\n",
    "methods = [\"pca\", \"tsne\", \"umap\"]\n",
    "reductions = {}\n",
    "\n",
    "for method in methods:\n",
    "    try:\n",
    "        # Get optimal parameters for the method\n",
    "        params = get_optimal_parameters(method, embedding_data.n_samples)\n",
    "        \n",
    "        # Perform reduction\n",
    "        reduction = reduce_dimensions(\n",
    "            embedding_data.vectors,\n",
    "            method=method,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        reductions[method] = reduction\n",
    "        \n",
    "        print(f\"{method.upper()}: Reduced to {reduction.n_components} dimensions\")\n",
    "        if reduction.explained_variance:\n",
    "            print(f\"  Explained variance: {reduction.explained_variance:.2%}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"{method.upper()}: Skipped - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{method.upper()}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Visualization\n",
    "\n",
    "Create a basic plot with collections distinguished by shape and color by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available reduction methods: ['pca', 'tsne']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available reduction methods: {list(reductions.keys())}\")# Use the first available reduction methodif reductions and len(reductions) > 0:    method_name, reduction = next(iter(reductions.items()))        # Create plot configuration    config = EmbeddingPlotConfig(        color_field=\"collection\",  # Color by collection        shape_field=\"collection\",  # Different shapes for each collection        hover_fields=[\"id\", \"title\", \"headline\", \"category\", \"topic\"],        title=f\"Document Embeddings ({method_name.upper()})\",        width=800,        height=600,        point_size=10,        opacity=0.8    )        # Create the plot    fig = plot_embeddings(        embedding_data=embedding_data,        reduction_result=reduction,        config=config    )        # Display the plot    # fig.show() # Commented for papermill testingelse:    print(\"No reduction methods available. Please install scikit-learn or umap-learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Visualization: Color by Metadata\n",
    "\n",
    "Let's create a more sophisticated visualization where we color points by their category/topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available reduction methods: ['pca', 'tsne']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available reduction methods: {list(reductions.keys())}\")# Add a unified category field for coloringfor i, meta in enumerate(embedding_data.metadata):    if \"category\" in meta:        meta[\"unified_category\"] = meta[\"category\"]    elif \"topic\" in meta:        meta[\"unified_category\"] = meta[\"topic\"]    else:        meta[\"unified_category\"] = \"Unknown\"if reductions and len(reductions) > 0:    # Create configuration with category coloring    config_advanced = EmbeddingPlotConfig(        color_field=\"unified_category\",  # Color by category/topic        shape_field=\"collection\",        # Shape by collection type        hover_fields=[\"id\", \"title\", \"headline\", \"unified_category\", \"year\", \"sentiment\"],        title=\"Document Embeddings by Category\",        width=900,        height=700,        point_size=12,        opacity=0.7,        color_discrete_map={            \"AI\": \"#FF6B6B\",            \"Climate\": \"#4ECDC4\",            \"Quantum\": \"#45B7D1\",            \"Technology\": \"#96CEB4\",            \"Finance\": \"#FECA57\",            \"Environment\": \"#48C9B0\",            \"Health\": \"#BB8FCE\"        }    )        fig_advanced = plot_embeddings(        embedding_data=embedding_data,        reduction_result=reduction,        config=config_advanced    )        # fig_advanced.show() # Commented for papermill testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Reduction Methods\n",
    "\n",
    "If multiple reduction methods are available, let's compare them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 2 method available. Need at least 2 for comparison.\n"
     ]
    }
   ],
   "source": [
    "if False and len(reductions) > 1:\n",
    "    # Prepare datasets for comparison\n",
    "    comparison_data = {\n",
    "        method: (embedding_data, reduction)\n",
    "        for method, reduction in reductions.items()\n",
    "    }\n",
    "    \n",
    "    # Create comparison plot\n",
    "    comparison_config = EmbeddingPlotConfig(\n",
    "        color_field=\"collection\",\n",
    "        title=\"Comparison of Dimensionality Reduction Methods\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        point_size=8\n",
    "    )\n",
    "    \n",
    "    fig_comparison = plot_embeddings_comparison(\n",
    "        embedding_datasets=comparison_data,\n",
    "        config=comparison_config\n",
    "    )\n",
    "    \n",
    "    # fig_comparison.show() # Commented for papermill testing\n",
    "else:\n",
    "    print(f\"Only {len(reductions)} method available. Need at least 2 for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Real Data from a Database\n",
    "\n",
    "Here's how you would use this with your actual bervo.ddb database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Connect to your database\\nclient = Client(\"duckdb:///~/databases/bervo.ddb\")\\ndb = client.get_database()\\n\\n# List available collections\\ncollections = db.list_collection_names()\\nprint(f\"Available collections: {collections}\")\\n\\n# Extract embeddings from your collections\\nembedding_data = extract_embeddings_from_multiple_collections(\\n    database=db,\\n    collection_names=collections[:2],  # Use first two collections\\n    index_name=\"llm\",  # or whatever index name you used\\n    limit_per_collection=1000,  # Limit for performance\\n    normalize=True\\n)\\n\\n# Perform UMAP reduction\\nreduction = reduce_dimensions(\\n    embedding_data.vectors,\\n    method=\"umap\",\\n    n_neighbors=15,\\n    min_dist=0.1,\\n    random_state=42\\n)\\n\\n# Create visualization\\nconfig = EmbeddingPlotConfig(\\n    color_field=\"your_property\",  # Replace with actual property\\n    shape_field=\"collection\",\\n    hover_fields=[\"id\", \"name\", \"description\"],  # Adjust to your schema\\n    title=\"Bervo Database Embeddings\",\\n    width=1000,\\n    height=800\\n)\\n\\nfig = plot_embeddings(\\n    embedding_data=embedding_data,\\n    reduction_result=reduction,\\n    config=config,\\n    output_file=\"bervo_embeddings.html\"\\n)\\n\\n    # fig.show() # Commented for papermill testing\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code for your actual database (uncomment to use)\n",
    "\"\"\"\n",
    "# Connect to your database\n",
    "client = Client(\"duckdb:///~/databases/bervo.ddb\")\n",
    "db = client.get_database()\n",
    "\n",
    "# List available collections\n",
    "collections = db.list_collection_names()\n",
    "print(f\"Available collections: {collections}\")\n",
    "\n",
    "# Extract embeddings from your collections\n",
    "embedding_data = extract_embeddings_from_multiple_collections(\n",
    "    database=db,\n",
    "    collection_names=collections[:2],  # Use first two collections\n",
    "    index_name=\"llm\",  # or whatever index name you used\n",
    "    limit_per_collection=1000,  # Limit for performance\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Perform UMAP reduction\n",
    "reduction = reduce_dimensions(\n",
    "    embedding_data.vectors,\n",
    "    method=\"umap\",\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "config = EmbeddingPlotConfig(\n",
    "    color_field=\"your_property\",  # Replace with actual property\n",
    "    shape_field=\"collection\",\n",
    "    hover_fields=[\"id\", \"name\", \"description\"],  # Adjust to your schema\n",
    "    title=\"Bervo Database Embeddings\",\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig = plot_embeddings(\n",
    "    embedding_data=embedding_data,\n",
    "    reduction_result=reduction,\n",
    "    config=config,\n",
    "    output_file=\"bervo_embeddings.html\"\n",
    ")\n",
    "\n",
    "    # fig.show() # Commented for papermill testing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Usage\n",
    "\n",
    "You can also use the command-line interface to generate these plots:\n",
    "\n",
    "```bash\n",
    "# Basic usage\n",
    "linkml-store -d ~/databases/bervo.ddb plot-embeddings \\\n",
    "  -c collection1,collection2 \\\n",
    "  --method umap \\\n",
    "  -o embeddings.html\n",
    "\n",
    "# Advanced usage with custom parameters\n",
    "linkml-store -d ~/databases/bervo.ddb plot-embeddings \\\n",
    "  -c collection1,collection2 \\\n",
    "  --method umap \\\n",
    "  --color-field category \\\n",
    "  --shape-field collection \\\n",
    "  --hover-fields id,name,description \\\n",
    "  --n-neighbors 30 \\\n",
    "  --min-dist 0.05 \\\n",
    "  --width 1200 \\\n",
    "  --height 900 \\\n",
    "  --dark-mode \\\n",
    "  -o embeddings_advanced.html\n",
    "\n",
    "# Using t-SNE instead\n",
    "linkml-store -d ~/databases/bervo.ddb plot-embeddings \\\n",
    "  -c collection1,collection2 \\\n",
    "  --method tsne \\\n",
    "  --perplexity 50 \\\n",
    "  --limit-per-collection 500 \\\n",
    "  -o embeddings_tsne.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Clustering\n",
    "\n",
    "Let's add clustering to identify groups in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reductions:\n",
    "    try:\n",
    "        from sklearn.cluster import KMeans\n",
    "        from linkml_store.plotting.embedding_plot import plot_embedding_clusters\n",
    "        \n",
    "        # Perform clustering on the reduced dimensions\n",
    "        n_clusters = 3\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(reduction.coordinates)\n",
    "        \n",
    "        # Create cluster visualization\n",
    "        cluster_config = EmbeddingPlotConfig(\n",
    "            shape_field=\"collection\",\n",
    "            hover_fields=[\"id\", \"title\", \"headline\", \"unified_category\"],\n",
    "            title=f\"Document Clusters (K-Means, k={n_clusters})\",\n",
    "            width=900,\n",
    "            height=700\n",
    "        )\n",
    "        \n",
    "        fig_clusters = plot_embedding_clusters(\n",
    "            embedding_data=embedding_data,\n",
    "            reduction_result=reduction,\n",
    "            cluster_labels=cluster_labels,\n",
    "            config=cluster_config\n",
    "        )\n",
    "        \n",
    "    # fig_clusters.show() # Commented for papermill testing\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"scikit-learn required for clustering. Install with: pip install scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Exporting Results\n",
    "\n",
    "Finally, let's save our results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot to HTML\n",
    "if reductions and 'fig' in locals():\n",
    "    output_file = \"embedding_visualization.html\"\n",
    "    fig.write_html(output_file)\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "    \n",
    "    # Export data for external analysis\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame with coordinates and metadata\n",
    "    export_data = {\n",
    "        \"x\": reduction.coordinates[:, 0],\n",
    "        \"y\": reduction.coordinates[:, 1],\n",
    "        \"collection\": embedding_data.collection_names,\n",
    "        \"id\": embedding_data.object_ids\n",
    "    }\n",
    "    \n",
    "    # Add metadata fields\n",
    "    for key in [\"title\", \"headline\", \"category\", \"topic\"]:\n",
    "        values = embedding_data.get_metadata_values(key)\n",
    "        if any(v is not None for v in values):\n",
    "            export_data[key] = values\n",
    "    \n",
    "    df = pd.DataFrame(export_data)\n",
    "    df.to_csv(\"embeddings_data.csv\", index=False)\n",
    "    print(f\"Data exported to embeddings_data.csv\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nData summary:\")\n",
    "    print(df.describe())\n",
    "    print(\"\\nCollection distribution:\")\n",
    "    print(df['collection'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. Extract embeddings from indexed LinkML-Store collections\n",
    "2. Apply dimensionality reduction (PCA, t-SNE, UMAP)\n",
    "3. Create interactive visualizations with different encoding schemes\n",
    "4. Compare multiple collections and reduction methods\n",
    "5. Add clustering to identify groups\n",
    "6. Export results for further analysis\n",
    "\n",
    "The embedding visualization tools help you:\n",
    "- Understand the structure of your indexed data\n",
    "- Identify clusters and patterns\n",
    "- Compare different collections\n",
    "- Debug indexing and search issues\n",
    "- Explore semantic relationships in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
